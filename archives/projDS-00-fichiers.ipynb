{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae75bdb8",
   "metadata": {},
   "source": [
    "# Notebook de constitution des fichiers de travail\n",
    "> Ce notebook n'a pour vocation que de constituer un jeu de données de travail df_global.<br>\n",
    "Il est basé sur les analyses faites dans le notebook `projDS-00-analyse.ipynb`, mais est beaucoup plus léger.<br><br>\n",
    "_NB : si l'on ne veut pas recréer les fichiers `df_sentiment.csv`, `df_usertrack.csv`, `df_context.csv` il suffit de passer directement à l'éxécution des cellules \"global\", ou alors d'éxécuter tout le notebook après avoir passé les cellules des 3 fichiers en mode 'texte brut' (Esc + R), puis les ramener en mode 'code' si besoin plus tard (Esc + Y)_\n",
    "\n",
    "Les choix peuvent évoluer au fil du temps et être modifiés par les intervenant(e)s sur le projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc4e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "begin_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cb05b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"hautdepage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20e636",
   "metadata": {},
   "source": [
    "### Table des matières\n",
    "\n",
    "- Fichiers d'origine => fichiers de travail\n",
    " - [sentiment_values.csv -> df_sentiment.csv (dictionnaires de sentiments)](#df_sentiment)\n",
    " - [user_track_hashtag_timestamp.csv -> df_usertrack.csv (données utilisateurs)](#df_usertrack)\n",
    " - [context_content_features.csv -> df_context.csv (spécificités des morceaux)](#df_context)\n",
    "\n",
    "\n",
    "- DataFrame global à partir des fichiers de travail\n",
    " - [df_global (version complete)](#df_global)\n",
    " - [df_global_light (version light)](#df_global_light) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c435b6",
   "metadata": {},
   "source": [
    "## Fichiers d'origine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50a1e1",
   "metadata": {},
   "source": [
    "<a id=\"df_sentiment\"></a>\n",
    "#### Dictionnaire de sentiments : sentiment_values.csv -> df_sentiment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ef76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5290 entries, 0 to 5289\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   hashtag      5290 non-null   object \n",
      " 1   sent_score2  5290 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 82.8+ KB\n",
      "\n",
      "tps d'exécution:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# lecture du fichier d'origine\n",
    "df_sentiment = pd.read_csv('sentiment_values.csv', sep=',')\n",
    "df_sentiment.dropna(axis = 0, how ='all', inplace=True)\n",
    "\n",
    "# traitements\n",
    "\n",
    "# --- colonnes\n",
    "df_sentiment.reset_index(inplace=True)\n",
    "new_cols = ['hashtag', 'vader_min', 'vader_max', 'vader_sum', 'vader_avg',\n",
    "       'afinn_min', 'afinn_max', 'afinn_sum', 'afinn_avg', 'ol_min', 'ol_max',\n",
    "       'ol_sum', 'ol_avg', 'ss_min', 'ss_max', 'ss_sum', 'ss_avg','X_min', 'X_max', 'X_sum', 'X_avg']\n",
    "df_sentiment.columns = new_cols\n",
    "\n",
    "# --- choix d'un dictionnaire \n",
    "# 1ere option : compléter les valeurs nulles dans l'ordre décroissant des variables nulles 'vader_avg', 'ss_avg', 'ol_avg', 'X_avg'\n",
    "df_sentiment['sent_score1'] = ((df_sentiment['vader_avg'].fillna(df_sentiment['ol_avg'])).fillna(df_sentiment['X_avg'])).fillna(df_sentiment['ss_avg'])\n",
    "\n",
    "# 2ème option : faire une moyenne de toutes les valeurs\n",
    "df_sentiment['sent_score2'] = df_sentiment[['vader_avg','ol_avg','X_avg','ss_avg']].mean(numeric_only=True, axis=1)\n",
    "\n",
    "# 3ème option : vader_avg ou la moyenne des autres variables\n",
    "df_sentiment['sent_score3'] = df_sentiment['vader_avg'].fillna(df_sentiment[['ol_avg','X_avg','ss_avg']].mean(numeric_only=True, axis=1))\n",
    "\n",
    "# écriture du fichier csv\n",
    "df_sentiment = df_sentiment[['hashtag', 'sent_score2']]\n",
    "df_sentiment.to_csv('df_sentiment.csv',index_label='idx_sent')\n",
    "\n",
    "df_sentiment.info()\n",
    "\n",
    "print(\"\\ntps d'exécution: \",timedelta(seconds=round((time.time() - start_time),0)))\n",
    "# tps d'éxécution:  0:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629601c",
   "metadata": {},
   "source": [
    "[retour en haut de page](#hautdepage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5abcb",
   "metadata": {},
   "source": [
    "<a id=\"df_usertrack\"></a>\n",
    "#### Données utilisateurs : user_track_hashtag_timestamp.csv -> df_usertrack.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b7b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6356936 entries, 1 to 17560112\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   user_id        6356936 non-null  int64 \n",
      " 1   track_id       6356936 non-null  object\n",
      " 2   hashtag        6356936 non-null  object\n",
      " 3   created_at     6356936 non-null  object\n",
      " 4   hashtag_found  6356936 non-null  int32 \n",
      "dtypes: int32(1), int64(1), object(3)\n",
      "memory usage: 266.7+ MB\n",
      "\n",
      "tps d'exécution:  0:01:08\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# lecture du fichier d'origine\n",
    "df_usertrack = pd.read_csv('user_track_hashtag_timestamp.csv')\n",
    "df_usertrack.dropna(axis = 0, how ='all', inplace=True)\n",
    "\n",
    "# traitements\n",
    "\n",
    "# --- suppression de l'entrée avec le hashtag = Nan\n",
    "df_usertrack.dropna(subset=['hashtag'], inplace=True)\n",
    "\n",
    "# --- conversion des hashtags en minuscule\n",
    "df_usertrack['hashtag'] = df_usertrack['hashtag'].str.lower()\n",
    "\n",
    "# --- hashtags trouvés ou pas dans le dictionnaire sentiment\n",
    "sentiment_hashtag = df_sentiment['hashtag']\n",
    "df_usertrack['hashtag_found'] = np.nan\n",
    "df_usertrack['hashtag_found'] = np.where(df_usertrack['hashtag'].isin(sentiment_hashtag), 1, 0)\n",
    "\n",
    "# --- suppression des hashtags n'apportant aucune information pertinente\n",
    "df_usertrack = df_usertrack[(df_usertrack['hashtag'] != \"nowpalying\") &\n",
    "                            (df_usertrack['hashtag'] != \"nowplay\") &\n",
    "                            (df_usertrack['hashtag'] != \"nowplayin\") &\n",
    "                            (df_usertrack['hashtag'] != \"nowplaying\") &\n",
    "                            (df_usertrack['hashtag'] != \"np\")\n",
    "                           ]\n",
    "\n",
    "# écriture du fichier csv\n",
    "df_usertrack.to_csv('df_usertrack.csv', index_label='idx_usrtrck')\n",
    "df_usertrack.info(show_counts=True)\n",
    "\n",
    "print(\"\\ntps d'exécution: \",timedelta(seconds=round((time.time() - start_time),0)))\n",
    "# tps d'éxécution:  0:02:11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488075e",
   "metadata": {},
   "source": [
    "[retour en haut de page](#hautdepage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02089f6e",
   "metadata": {},
   "source": [
    "<a id=\"df_context\"></a>\n",
    "#### Spécificités des morceaux : context_content_features.csv -> df_context.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df13a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11614671 entries, 0 to 11614670\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count     Dtype  \n",
      "---  ------                --------------     -----  \n",
      " 0   instrumentalness      11614671 non-null  float64\n",
      " 1   liveness              11614671 non-null  float64\n",
      " 2   speechiness           11614671 non-null  float64\n",
      " 3   danceability          11614671 non-null  float64\n",
      " 4   valence               11614671 non-null  float64\n",
      " 5   loudness              11614671 non-null  float64\n",
      " 6   tempo                 11614671 non-null  float64\n",
      " 7   acousticness          11614671 non-null  float64\n",
      " 8   energy                11614671 non-null  float64\n",
      " 9   artist_id             11614671 non-null  object \n",
      " 10  track_id              11614671 non-null  object \n",
      " 11  created_at            11614671 non-null  object \n",
      " 12  user_id               11614671 non-null  int64  \n",
      " 13  user_id_found         11614671 non-null  int32  \n",
      " 14  user_track_found      11614671 non-null  int32  \n",
      " 15  user_createdat_found  11614671 non-null  int32  \n",
      " 16  region                11614671 non-null  object \n",
      "dtypes: float64(9), int32(3), int64(1), object(4)\n",
      "memory usage: 1.7+ GB\n",
      "\n",
      "tps d'exécution:  0:06:04\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# lecture du fichier d'origine\n",
    "df_context = pd.read_csv('context_content_features.csv', usecols=range(0, 22))\n",
    "df_context.dropna(axis = 0, how ='all', inplace=True);\n",
    "\n",
    "# traitements\n",
    "# conversion du user_id au format int + attribution de la valeur 0 aux user_id = NaN\n",
    "df_context['user_id'].fillna(0, inplace=True)\n",
    "df_context['user_id'] = df_context['user_id'].astype('int64')\n",
    "\n",
    "# --- user_id, track_id et created_at qui existent ou pas dans le fichier user_track_hashtag_timestamp\n",
    "usr_user_id = df_usertrack['user_id']\n",
    "usr_track_id = df_usertrack['track_id']\n",
    "usr_created_at = df_usertrack['created_at']\n",
    "\n",
    "df_context['user_id_found'] = np.nan\n",
    "df_context['user_track_found'] = np.nan\n",
    "df_context['user_createdat_found'] = np.nan\n",
    "\n",
    "df_context['user_id_found'] = np.where(df_context['user_id'].isin(usr_user_id), 1, 0)\n",
    "df_context['user_track_found'] = np.where(df_context['track_id'].isin(usr_track_id), 1, 0)\n",
    "df_context['user_createdat_found'] = np.where(df_context['created_at'].isin(usr_created_at), 1, 0)\n",
    "\n",
    "\n",
    "# --- regroupement par régions\n",
    "df_context['region'] = np.nan\n",
    "df_context['time_zone'].fillna(\"\", inplace=True)\n",
    "\n",
    "reg = \"America\"\n",
    "lst = \"US|USA|America|Alaska|Hawaii|Honolulu|Atlantic Time|Arizona|Indiana|Midway Island|Mid-Atlantic|Saskatchewan|Newfoundland\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "reg = \"Europe\"\n",
    "lst = \"EU|Europe|EST|Greenland|Kyiv|Belgrade|Yerevan|Warsaw|Sofia|Riga|Azores|Ljubljana|Volgograd|Sarajevo|Skopje|Bucharest|Vladivostok|Magadan|Brussels|Tallinn|Kamchatka|Novosibirsk|Krasnoyarsk|Bratislava|Zagreb|Vilnius|Yakutsk|Prague|Minsk|Copenhagen|Petersburg|Ekaterinburg|Irkutsk|Vienna|Budapest|Paris|Moscow|Lisbon|Bern|London|Amsterdam|Athens|Stockholm|Helsinki|Dublin|Madrid|Rome|Berlin|Edinburgh\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "reg = \"Asia\"\n",
    "lst = \"Tokyo|Asia|Hanoi|Chennai|Mumbai|New Delhi|Singapore|Kathmandu|Urumqi|JST|Osaka|Beijing|Rangoon|Chongqing|Sapporo|Port Moresby|Solomon Is.|Jakarta|Bangkok|Guam|Kolkata|Ulaan Bataar|Kuala|Hong Kong|Taipei|Seoul|Sri Jayawardenepura\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "reg = \"Latine-America\"\n",
    "lst = \"Monterrey|Lima|Bogota|Guadalajara|La Paz|Mazatlan|Chihuahua|Georgetown|Santiago|Brasilia|Quito|Buenos Aires|Caracas|Mexico City|Tijuana\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "reg = \"Middle-East\"\n",
    "lst = \"Tehran|Baghdad|Cairo|Casablanca|Istanbul|Tbilisi|Jerusalem|Kabul|Karachi|Muscat|Abu Dhabi|Kuwait|Dhaka|Baku|Tashkent|Almaty|Astana|Riyadh|Islamabad\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "reg = \"Africa\"\n",
    "lst = \"Pretoria|Africa|Harare|Nairobi|Cape Verde Is.|Monrovia\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "reg = \"Oceania\"\n",
    "lst = \"Perth|Melbourne|Brisbane|Sydney|New Caledonia|Adelaide|Wellington|Marshall Is.|Nuku'alofa|Canberra|Auckland|Wellington|Hobart|Samoa|Fiji|Darwin\"\n",
    "idx = df_context[df_context['time_zone'].str.contains(lst)].index\n",
    "df_context.loc[idx,'region'] = reg\n",
    "del lst, reg, idx\n",
    "\n",
    "df_context['region'].fillna(\"undefined\", inplace=True)\n",
    "\n",
    "\n",
    "# --- spécificités des morceaux\n",
    "df_context['instrumentalness'].fillna(0, inplace=True)\n",
    "df_context['liveness'].fillna(0, inplace=True)\n",
    "df_context['speechiness'].fillna(0, inplace=True)\n",
    "df_context['danceability'].fillna(0, inplace=True)\n",
    "df_context['valence'].fillna(0, inplace=True)\n",
    "df_context['loudness'].fillna(0, inplace=True)\n",
    "df_context['tempo'].fillna(0, inplace=True)\n",
    "df_context['acousticness'].fillna(0, inplace=True)\n",
    "df_context['energy'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# --- suppression des variables sans intérêt\n",
    "df_context.drop(columns=['lang', 'tweet_lang', 'coordinates', 'geo', 'place', 'time_zone', 'id', 'mode', 'key'], inplace=True)\n",
    "\n",
    "\n",
    "# écriture du fichier csv\n",
    "df_context.to_csv('df_context.csv', index_label='idx_ctxt')\n",
    "df_context.info(show_counts=True)\n",
    "\n",
    "print(\"\\ntps d'exécution: \",timedelta(seconds=round((time.time() - start_time),0)))\n",
    "# tps d'éxécution:  0:07:06\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3203a59",
   "metadata": {},
   "source": [
    "[retour en haut de page](#hautdepage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e359e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tps d'exécution des 3 fichiers:  0:07:13\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntps d'exécution des 3 fichiers: \",timedelta(seconds=round((time.time() - begin_time),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3976f",
   "metadata": {},
   "source": [
    "## Fichier global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f59fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tps d'exécution:  0:01:32\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# lecture des fichiers de travail \n",
    "df_sentiment = pd.read_csv(\"df_sentiment.csv\")\n",
    "df_context = pd.read_csv(\"df_context.csv\")\n",
    "df_usertrack = pd.read_csv(\"df_usertrack.csv\")\n",
    "\n",
    "print(\"\\ntps d'exécution: \",timedelta(seconds=round((time.time() - start_time),0)))\n",
    "# tps d'exécution:  0:02:13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de577c92",
   "metadata": {},
   "source": [
    "<a id=\"df_global\"></a>\n",
    "#### Version complète : df_global.csv\n",
    "\n",
    "Enrichissement du fichier des utilisateurs `df_user (user_track_hashtag_timestamp)`\n",
    "- par les sentiments : `df_sentiment (sentiment_values)` -> merge sur le hashtag\n",
    "- puis par les spécificités du morceau : `df_context (context_content_features)` -> merge sur le track_id (dédoublonnage des track_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b8efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6356931 entries, 0 to 6356930\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   index             6356931 non-null  int64  \n",
      " 1   user_id           6356931 non-null  int64  \n",
      " 2   track_id          6356931 non-null  object \n",
      " 3   artist_id         6356931 non-null  object \n",
      " 4   created_at        6356931 non-null  object \n",
      " 5   instrumentalness  6356931 non-null  float64\n",
      " 6   liveness          6356931 non-null  float64\n",
      " 7   speechiness       6356931 non-null  float64\n",
      " 8   danceability      6356931 non-null  float64\n",
      " 9   valence           6356931 non-null  float64\n",
      " 10  loudness          6356931 non-null  float64\n",
      " 11  tempo             6356931 non-null  float64\n",
      " 12  acousticness      6356931 non-null  float64\n",
      " 13  energy            6356931 non-null  float64\n",
      " 14  region            6356931 non-null  object \n",
      " 15  hashtag           6356931 non-null  object \n",
      " 16  sent_score        431906 non-null   float64\n",
      " 17  hashtag_found     6356931 non-null  int64  \n",
      "dtypes: float64(10), int64(3), object(5)\n",
      "memory usage: 873.0+ MB\n",
      "\n",
      "tps d'exécution:  0:02:13\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# --- merge de user_track avec le dictionnaire de sentiment\n",
    "df_global = df_usertrack.merge(right = df_sentiment, on = 'hashtag', how = 'left')\n",
    "\n",
    "# renommage colonne score\n",
    "df_global.rename(columns={\"sent_score2\": \"sent_score\"}, inplace=True)\n",
    "\n",
    "\n",
    "# --- merge avec spécificités du morceau \n",
    "\n",
    "# dedoublonnage des morceaux (track_id)\n",
    "# en suppprimant les infos déjà présentes dans df_usertrack : created_at & user_id\n",
    "# et les infos non essentielles 'user_id_found', 'user_track_found', 'user_createdat_found'\n",
    "df_context_cleaned = df_context.drop_duplicates(subset=['track_id']).drop(columns=['created_at', 'user_id', 'user_id_found', 'user_track_found', 'user_createdat_found'])\n",
    "# merge\n",
    "df_global = df_global.merge(right = df_context_cleaned, on = 'track_id', how = 'left')\n",
    "\n",
    "# ordre des colonnes + logique\n",
    "# ( + suppression des colonnes d'index des fichiers de travail : idx_sent & idx_usrtrck )\n",
    "# ( + suppression des entrées spécificités + hashtag nulles )\n",
    "df_global = df_global[['user_id', 'track_id', 'artist_id', 'created_at', \n",
    "                      'instrumentalness', 'liveness', 'speechiness', 'danceability', 'valence', 'loudness', 'tempo', 'acousticness', 'energy',\n",
    "                      'region', 'hashtag', 'sent_score',\n",
    "                       'hashtag_found'\n",
    "                      ]].dropna(subset=['liveness', 'hashtag']).reset_index()\n",
    "\n",
    "# écriture du fichier csv\n",
    "df_global.to_csv('df_global.csv',index=False)\n",
    "df_global.info(show_counts=True)\n",
    "\n",
    "print(\"\\ntps d'exécution: \",timedelta(seconds=round((time.time() - start_time),0)))\n",
    "# tps d'exécution:  0:02:13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a63fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5925025\n",
       "1     431906\n",
       "Name: hashtag_found, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global['hashtag_found'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3cf4d",
   "metadata": {},
   "source": [
    "[retour en haut de page](#hautdepage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e4bd6",
   "metadata": {},
   "source": [
    "<a id=\"df_global_light\"></a>\n",
    "#### Version \"light\" : df_global_light.csv\n",
    "\n",
    "Le fichier même fichier que précédemment `df_global.csv` mais avec uniquement les entrées comportant des scores de sentiment (hashtags présents dans le fichier `df_user - user_track_hashtag_timestamp`) et en supprimant la variables devenue inutile 'hashtag_found'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7151463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 431906 entries, 1 to 6356910\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   index             431906 non-null  int64  \n",
      " 1   user_id           431906 non-null  int64  \n",
      " 2   track_id          431906 non-null  object \n",
      " 3   artist_id         431906 non-null  object \n",
      " 4   created_at        431906 non-null  object \n",
      " 5   instrumentalness  431906 non-null  float64\n",
      " 6   liveness          431906 non-null  float64\n",
      " 7   speechiness       431906 non-null  float64\n",
      " 8   danceability      431906 non-null  float64\n",
      " 9   valence           431906 non-null  float64\n",
      " 10  loudness          431906 non-null  float64\n",
      " 11  tempo             431906 non-null  float64\n",
      " 12  acousticness      431906 non-null  float64\n",
      " 13  energy            431906 non-null  float64\n",
      " 14  region            431906 non-null  object \n",
      " 15  hashtag           431906 non-null  object \n",
      " 16  sent_score        431906 non-null  float64\n",
      "dtypes: float64(10), int64(2), object(5)\n",
      "memory usage: 59.3+ MB\n",
      "\n",
      "tps d'exécution:  0:00:06\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_global_light = df_global[df_global['hashtag_found'] == 1].drop(columns=['hashtag_found'])\n",
    "\n",
    "# écriture du fichier csv\n",
    "df_global_light.to_csv('df_global_light.csv',index=False)\n",
    "df_global_light.info(show_counts=True)\n",
    "\n",
    "print(\"\\ntps d'exécution: \",timedelta(seconds=round((time.time() - start_time),0)))\n",
    "# tps d'exécution:  0:00:16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12435ca",
   "metadata": {},
   "source": [
    "[retour en haut de page](#hautdepage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3166d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tps d'éxécution total:  0:11:04\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntps d'éxécution total: \",timedelta(seconds=round((time.time() - begin_time),0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
